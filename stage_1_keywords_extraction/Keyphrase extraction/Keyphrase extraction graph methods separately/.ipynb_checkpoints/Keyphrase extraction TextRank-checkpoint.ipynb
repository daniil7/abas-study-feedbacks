{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8bfffe-913b-460a-81ce-e117dd928293",
   "metadata": {},
   "source": [
    "# Keyphrase extraction графовыми методами TextRank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab87422-a850-45f9-810d-3849ba71d10f",
   "metadata": {},
   "source": [
    "## Импорт данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd1e078-6869-40c2-bb64-a3567ef62c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87809d1a-f67a-4bf7-bf7b-f99ebbddf1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(\"dest/all_dataset.csv\", sep=\";\")\n",
    "all_df = all_df[all_df[\"rubrics\"].str.contains('Образование_онлайн_курсы')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3031d2-18d7-47ef-8bd8-60e306df960a",
   "metadata": {},
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0f9dd6-41a6-4ef8-8da4-c7ecda8c02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytextrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccdf61ed-0688-4d16-a3d9-ac59378f573b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0556     3  Курс\n",
      "[Курс, Курс, Курс]\n",
      "0.0556     8  Курсом\n",
      "[Курсом, Курсом, Курсом, Курсом, Курсом, Курсом, Курсом, Курсом]\n",
      "0.0387     1  Преподаватель Фрицлер\n",
      "[Преподаватель Фрицлер]\n",
      "0.0373     1  IT-компаний\n",
      "[IT-компаний]\n",
      "0.0363    46  Электив\n",
      "[Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив, Электив]\n",
      "0.0357     3  IT-компании\n",
      "[IT-компании, IT-компании, IT-компании]\n",
      "0.0356     1  Замечательный электив\n",
      "[Замечательный электив]\n",
      "0.0345     1  Шикарный электив\n",
      "[Шикарный электив]\n",
      "0.0333     1  Тюменских панк-рок групп\n",
      "[Тюменских панк-рок групп]\n",
      "0.0331     1  ДЗ - просмотр коротких фильмов\n",
      "[ДЗ - просмотр коротких фильмов]\n",
      "0.0327     1  Могла\n",
      "[Могла]\n",
      "0.0310     1  джуниор фронтенд-разработчиком\n",
      "[джуниор фронтенд-разработчиком]\n",
      "0.0301     1  Лучше\n",
      "[Лучше]\n",
      "0.0301     1  Хорошая\n",
      "[Хорошая]\n",
      "0.0301     1  Хороший\n",
      "[Хороший]\n",
      "0.0297     1  Студентам\n",
      "[Студентам]\n",
      "0.0282     1  Лекции\n",
      "[Лекции]\n",
      "0.0273     1  Я. Делает\n",
      "[Я. Делает]\n",
      "0.0261     3  Баллы\n",
      "[Баллы, Баллы, Баллы]\n",
      "0.0241     1  IT-компаниях\n",
      "[IT-компаниях]\n"
     ]
    }
   ],
   "source": [
    "# python -m spacy download ru_core_web_sm | ru_core_web_lg\n",
    "\n",
    "text = \" \".join(list(all_df['text'].dropna().astype(\"str\")))\n",
    "# text = \" \".join(list(otzyvus_df['text'].dropna().astype(\"str\")))\n",
    "\n",
    "# load a spaCy model, depending on language, scale, etc.\n",
    "nlp = spacy.load(\"ru_core_news_lg\")\n",
    "\n",
    "# add PyTextRank to the spaCy pipeline\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "doc = nlp(text[:1000000])\n",
    "# doc = nlp(text[1000000:2000000])\n",
    "# doc = nlp(text[2000000:3000000])\n",
    "\n",
    "# examine the top-ranked phrases in the document\n",
    "for p in doc._.phrases[:20]:\n",
    "    print(\"{:.4f} {:5d}  {}\".format(p.rank, p.count, p.text))\n",
    "    print(p.chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f45935-815c-49ca-8f22-b84f95200a01",
   "metadata": {},
   "source": [
    "## Ещё TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b565923-0919-437d-9f6d-24b3ed76fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from spacy.lang.ru.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d253092-44e4-44f0-9a4f-e2461e775ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_lg')\n",
    "\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ca53f0-ecb5-4091-be46-a97f102d36d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(list(all_df['text'].dropna().astype(\"str\")))[:1000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54e29b-978f-421b-acfb-1b2970dac4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, candidate_pos = ['NOUN', 'PROPN'], window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e62fbb4-ff45-4709-93d8-7a193397d50f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
